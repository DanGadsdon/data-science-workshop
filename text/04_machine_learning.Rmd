---
title: "Machine Learning: Document Classification and Topic Modeling"
author: Alex Hanna
date: January 21, 2016
output: html_document
---

[&laquo; Text Analysis Module](../text/README.md)

Machine learning can be defined as a set of probabilistic methods that can automatically detect patterns in data and use that information to make predictions in other data. Machine learning methods are at the center of most data science machinery, including those which have to do with text. 

We are going to explore two types of machine learning methods in this section: document classification and topic modeling. Document classification is a best used when you know the categories you are trying to predict, while topic modeling better-suited when you do not know these categories. This image from Grimmer and Stewart (2013) is a good visual for where the divisions occur.

![](Grimmer_Stewart_F1.png)

## Document Classification

Document classification is in a class of methods known as "supervised machine learning," which just means that there are humans "supervising" the process of annotation with known categories. This process involves "training" a classifier on a subset of documents, then using that classifier to classify a "test" set of documents. 

First, load the `tm` package which we used in the last section, as well as the `RTextTools` package, which has documentation [here](https://cran.r-project.org/web/packages/RTextTools/RTextTools.pdf). `RTextTools` provides functions for document classification, as well some handy functions which simplifies some of the preprocessing we did with `tm`.

The function `create_table` makes a `DocumentTermMatrix` and applies a number of preprocessing transformations by default, including converting to lowercase and stripping whitespace. We will also specify that we want stemming, that we want to keep in punctuation (because emoticons :-D >:( can be important for classification), and we want to use the tf-idf weighting.

```{r message=FALSE}
library(tm)
# install.packages("RTextTools")
library(RTextTools)

df.tweets <- read.csv("bullying.csv", stringsAsFactors = FALSE)
dtm       <- create_matrix(df.tweets$text, language="english", stemWords = TRUE, weighting = weightTfIdf, removePunctuation = FALSE)
dtm
```

We will replace "y" with "1" and "n" with "0" for the `bullying_traces` column, which indicates whether that particular tweet refers to potential bullying. Convert the column to a `numeric` type.

```{r}
df.tweets$bullying_traces[df.tweets$bullying_traces == 'y'] <- 1
df.tweets$bullying_traces[df.tweets$bullying_traces == 'n'] <- 0
df.tweets$bullying_traces <- as.numeric(df.tweets$bullying_traces)
```

We will use 90% of the data for the training set and 10% for the test set. Then, we will create a container which can be used with `RTextTools` models.

```{r}
training_break <- as.integer(0.9*nrow(df.tweets))
container      <- create_container(dtm, t(df.tweets$bullying_traces), trainSize=1:training_break, testSize=training_break:nrow(df.tweets), virgin=FALSE)
```

Now we can train and cross-validate a supervised learning model. We can see which ones are available with print_algorithms().

```{r}
print_algorithms()
```

Ideally, we would play around with multiple classifiers and find the one which works the best for the task. But for this workshop, we will choose the [Support Vector Machine](https://en.wikipedia.org/wiki/Support_vector_machine) (SVM). 

Cross-validation is a process of breaking up the training set into _k_ folds, using _k - 1_ folds as a training set and testing it against the last fold. We will use 3 folds. 

Accuracy is a measure defined as true positives + true negatives / the total population of items. It is equivalent to the proportion of correctly predict cases, and considered here a rough metric of model performance.

```{r}
cv.svm <- cross_validate(container, 3, algorithm = 'SVM', kernel = 'linear')
cv.svm$meanAccuracy
```

How good is this result? One way to think about is to compare with different random benchmarks. Since we have two categories, one benchmark would be 50%. Another one would be always choosing the modal category.

```{r}
prop.table(table(df.tweets$bullying_traces))
```

The metrics of [precision, recall, and F-score](https://en.wikipedia.org/wiki/Precision_and_recall) are somewhat more descriptive than accuracy. _Precision_ is the proportion of classified documents which are selected (\% correct positives), while _recall_ is the number of relevant documents which have been selected (\% true values correctly predicted). 

In the supervised learning example, a high precision indicates that the classifier can correctly classify documents without many false hits, while a high recall indicates that it can retrieve a high number of correctly classifed documents. _F1-score_ is the harmonic mean of the two metrics.

Now we can train the model, apply the trained model to our test set, and create analytics. 
```{r}
models    <- train_model(container, algorithms = c("SVM"))
results   <- classify_model(container, models)
analytics <- create_analytics(container, results)
```

Once we have trained the model and applied it, we can produce precision, recall, and F-score measures with `create_analytics`. We can also see which label the model applied, the correct label, and the confidence score which was assigned to it.

```{r message = FALSE}
# performance
analytics@algorithm_summary
apply(analytics@algorithm_summary, 2, mean)
# predicted labels in training dataset
head(analytics@document_summary[1:3])
```

## Topic Modeling

While supervised learning is used when we know the categories we want to produce, unsupervised learning (including topic modeling) is used when we do not know the categories. In topic modeling, documents are not assumed to belong to one topic or category, but simultaneously belong to several topics. The topic distributions also vary over documents.

For the topic model, we will focus only on messages which are in English.

```{r message = FALSE}

```