---
title: "Introduction to R text analysis"
author: Alex Hanna, Pablo Barbera, Dan Cervone
date: January 21, 2016
output: html_document
---

[&laquo; Text Analysis Module](../text/README.md)

In this section we are going to focus on some basic string manipulation in R. We will be using some of the R base functionality as well as the `stringr` package. For more information on the `stringr` package, check out the [documentation vignette](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html).

R stores the basic string in a character vector. `length` gets the number of items in the vector, while `nchar` is the number of characters in the vector.

```{r}
#install.packages("stringr")
library(stringr)
str1 <- 'This is a string.'
is.character(str1)
length(str1)
nchar(str1)
```

We can also have multiple strings within one vector. A vector of strings is the most simple kind of data structure for storage.

```{r}
v_str1 <- c('This is a string.', 'This is also a string.', 'A third one goes here.')
length(v_str1)
nchar(v_str1)
sum(nchar(v_str1))
```

The `str_to_*` functions convert case of a string.

```{r}
str_to_lower("Converting To Lowercase.")
str_to_upper("Converting To Uppercase.")
str_to_title("capitalize the first letter of every word.")
```

We can grab substrings with `substr`. The first argument is the string, the second is the beginning index (starting from 1), and the third is final index.

```{r}
substr(str1, 11, 16)
substr(str1, 1, 4)
```

This is useful when working with date strings as well:

```{r}
dates <- c("2015/01/01", "2014/12/01")
substr(dates, 1, 4) # years
substr(dates, 6, 7) # months
```

We can split up strings by a separator using `strsplit`.

```{r}
strsplit(str1, " ")
strsplit(str1, "s ")
```

It can also be part of a larger data frame of information. For this, load the dataset of tweets from the bullying project. Use `head` to look at the first few lines. 

```{r}
df.tweets <- read.csv("bullying.csv", header = TRUE, stringsAsFactors = FALSE)
str(df.tweets)
is.character(df.tweets$text)
head(df.tweets$text)
```

The text has gone some preprocessing for anonymity -- @usernames have been replaced with @USER and links starting with `http` have been replaced by HTTPLINK. 

Now we can start to dig into the text a little bit more. Given the construction of the dataset, we can expect that there will be many instances of the word "bully." We can use the `grep` command to identify these. `grep` returns the index where the word occurs.

```{r}
grep('bully', 'That guy is a buly.')
grep('bully', 'That guy is a bully. Fixed that for you.')
grep('bully', c('That guy is a buly.', 'That guy is a bully. Fixed that for you.', 'How dare you correct me.'))
```

`grepl` returns `TRUE` or `FALSE`, indicating whether each element of the character vector contains that particular pattern.

```{r}
grepl('bully', 'That guy is a buly.')
grepl('bully', 'That guy is a bully. Fixed that for you.')
grepl('bully', c('That guy is a buly.', 'That guy is a bully. Fixed that for you.', 'How dare you correct me.'))
```

Within the dataset, we can use the results of `grep` to get particular rows. First, check if the number of tweets mentioning "bully" match the total number of rows in the data frame.
```{r}
nrow(df.tweets)
grep('bully', df.tweets$text)
length(grep('bully', df.tweets$text))

grepl('bully', df.tweets$text)
```

We are not retrieving all of the items within the dataset. One reason is because matching is case-sensitive. You can use the `ignore.case` argument to match to a lowercase version. You can also create a new column in which everything is lowercase with `str_to_lower`.

```{r}
nrow(df.tweets)
length(grep('bully', df.tweets$text, ignore.case = TRUE))
df.tweets$lower_text <- str_to_lower(df.tweets$text)
length(grep('bully', df.tweets$lower_text))
```

We still are not capturing all the items. We can use what are called "regular expressions" to detect strings in a more robust manner. You can learn more about regular expressions [here](http://www.zytrax.com/tech/web/regex.htm). Regular expressions let us develop complicated rules for both matching strings and extracting elements from them. 

According to the documentation in the bullying project, they collected tweets contains at least one of the following keywords: "bully, bullied, bullying." One of the ways to do this, then, would be to try to match for the word "bull" plus any of the three endings: -y, -ied, -ying.

```{r}
nrow(df.tweets)
length(grep('bull(y|ied|ying)', df.tweets$lower_text))
```

This still doesn't get all the tweets. Let's try the much more inclusive `+` operator. This denotes that we want to match one or more of the previous character. We can use this along with all lowercase letters in the Latin alphabet by denoting a "character class" with brackets. So all lowercase letters would be `[a-z]`.

```{r}
nrow(df.tweets)
length(grep('bull[a-z]+', df.tweets$lower_text))
```

That did the trick! Other common expression patterns are:

- `.` matches any character, `^` and `$` match the beginning and end of a string.  
- Any character followed by `{3}`, `*`, `+` is matched exactly 3 times, 0 or more times, 1 or more times.  
- `[0-9]`, `[a-zA-Z]`, `[:alnum:]` match any digit, any letter, or any digit and letter.
- Special characters such as `.`, `\`, `(` or `)` must be preceded by a backslash.  
- See `?regex` for more details.

Another function that we will use is `gsub`, which replaces a pattern (or a regular expression) with another string:

```{r}
string <- c("Today is January 21st", "Yesterday was the 20th")
gsub('[0-9]+', 'ZZ', string)
gsub('[A-Z]+', 'CAPITALLETTER', string)
gsub('t$', 'FINAL-T', string)
```

To extract a pattern, and not just replace, use parentheses and choose the option `repl="\\1"`:

```{r}
string <- c("Today is January 21st", "Yesterday was the 20th")
gsub('.*([0-9]{2}).*', string, repl="\\1")
```

Now let's try to identify what tweets are probably not related to bullying. For example, how many tweets mention `bullet`?

```{r}
nrow(df.tweets)
length(grep('bullet', df.tweets$lower_text))
```

Let's assume we want to get rid of these. How would we do it? First, let's create a new column to the data frame that has value `TRUE` for tweets that mention this keyword and `FALSE` otherwise. Then, we can keep the rows with value `TRUE`.

```{r}
df.tweets$bullet <- grepl('bullet', df.tweets$text, ignore.case=TRUE)
table(df.tweets$bullet)
df.tweets.subset <- df.tweets[df.tweets$bullet==FALSE, ]
```

Finally, say we want to extract the word that actually refers to the bullying in the tweet. We can use `str_extract` and `str_extract_all` to do that with the regular expression we created later. Using `str_extract` will only get the first instance, while `str_extract_all` will get the whole list.

Store the output of `str_extract_all` in a new column called `bully_instance`. 

```{r}
head(str_extract(df.tweets$lower_text, 'bull[a-z]+'), 15)
head(str_extract_all(df.tweets$lower_text, 'bull[a-z]+'), 15)
df.tweets$bully_instance <- str_extract_all(df.tweets$lower_text, 'bull[a-z]+')
```

We can see the instance in context with `dplyr` and the `select` function.

```{r message = FALSE,}
library(dplyr)
df.tweets[13:15,] %>% select(text, bully_instance)
```

What does the function do when there are more than one items extracted?

Alternatively, we can also collapse the different words mentioned ourselves.

```{r}
mentions <- str_extract_all(df.tweets$lower_text, 'bull[a-z]+')
mentions[10:15] # a list

paste(c("bully", "bullying", "bullied"), collapse=", ")

mentions <- lapply(mentions, paste, collapse=", ")
mentions[10:15] # a list
df.tweets$bully_instance <- unlist(mentions) # a vector
df.tweets[13:15, c("text", "bully_instance")]
```



#### Dictionary methods

A different type of keyword analysis consists on the application of dictionary methods, or lexicon-based approaches to the measurement of tone or the prediction of diferent categories related to the content of the text. 

The most common application is sentiment analysis: using a dictionary of positive and negative words, we compute a sentiment score for each individual document.

```{r}
# loading lexicon of positive and negative words (from Neal Caren)
lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
pos.words <- lexicon$word[lexicon$polarity=="positive"]
neg.words <- lexicon$word[lexicon$polarity=="negative"]
# a look at a random sample of positive and negative words
sample(pos.words, 10)
sample(neg.words, 10)
```

We will need to preprocess the text a bit. We'll see more details about this soon, but for now let's just split into words using spaces and remove punctuation:

```{r}
clean_text <- function(text){
  text <- gsub('[[:punct:]]+', '', text) # remove punctuation
  words <- str_split(text, " +") # one or more spaces
  return(words)
}
# let's see if it works...
df.tweets$lower_text[1]
clean_text(df.tweets$lower_text[1])

```

And now we can write a function to classify the words in each tweet:
```{r}
# a function to classify individual tweets
classify <- function(words, pos.words, neg.words){
    # count number of positive and negative word matches
    pos.matches <- sum(words %in% pos.words)
    neg.matches <- sum(words %in% neg.words)
    return(pos.matches - neg.matches)
}
# an example
(text <- clean_text(df.tweets$lower_text[2]))
classify(text[[1]], pos.words, neg.words)

# finding what words were classified as positive/negative
words <- clean_text(df.tweets$lower_text[2])
words <- unlist(words)

words[words %in% pos.words]
words[words %in% neg.words]

```

But we want to aggregate over many tweets, so let's write a function that works with a list of character strings:

```{r}
sentiment_scores <- function(text, pos.words, neg.words){
    # classifier
    scores <- unlist(lapply(text, classify, pos.words, neg.words))
    n <- length(scores)
    positive <- as.integer(length(which(scores>0))/n*100)
    negative <- as.integer(length(which(scores<0))/n*100)
    neutral <- 100 - positive - negative
    cat(n, "tweets:", positive, "% positive,",
        negative, "% negative,", neutral, "% neutral")
}
```

And let's run everything:

```{r}
# clean all tweets
text <- clean_text(df.tweets$lower_text)
length(text); class(text)
# compute aggregate sentiment
sentiment_scores(text, pos.words, neg.words)
```

What do we learn?
